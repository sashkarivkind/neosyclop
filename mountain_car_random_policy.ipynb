{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f713d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this notebook is dedicated for finding appropriate parameters for a random policy\\nwich will then be used as a starting point in a policy gradient tuning and debug'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''this notebook is dedicated for finding appropriate parameters for a random policy\n",
    "wich will then be used as a starting point in a policy gradient tuning and debug'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b430abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "from imagenet_dataset import get_dataset\n",
    "from retina_env import RetinaEnv, calculate_retinal_filter\n",
    "from rl_networks import create_actor_model, create_critic_model, policy\n",
    "from rl_core import Buffer, update_target\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pg_core import GaussianPolicyAgent\n",
    "from rl_networks import create_actor_network_test_v1\n",
    "\n",
    "from test_envs import SimpleMountainCarContinuous \n",
    "from misc import print_progress_bar\n",
    "import types\n",
    "config = types.SimpleNamespace()\n",
    "config.batch_size = 1\n",
    "# config.margin = 20\n",
    "# config.image_h = 224\n",
    "# config.image_w = 224\n",
    "# config.image_hm = config.image_h+2*config.margin\n",
    "# config.image_wm = config.image_w+2*config.margin\n",
    "# config.foveate = None\n",
    "# config.do_grayscale = True\n",
    "# config.history_length = 16\n",
    "# config.t_ignore = 16\n",
    "# config.t_max =50\n",
    "# config.motion_mode = 'velocity'\n",
    "# config.use_dones = False\n",
    "\n",
    "# config.gym_mode = False\n",
    "# t_vec = np.linspace(0,150,16)\n",
    "\n",
    "# balanced_filter = calculate_retinal_filter(t_vec, R=1.0)\n",
    "# config.filter = balanced_filter.reshape([1,1,-1,1])\n",
    "# config.min_freq = 1\n",
    "# config.max_freq = 13\n",
    "# config.action_upper_bound = np.array([2.0, 2.0])\n",
    "actor_lr = 1e-4\n",
    "dataset_dir = '/home/bnapp/datasets/tensorflow_datasets/imagenet2012/5.0.0/'\n",
    "\n",
    "def epsilon_scheduler(episode, floor_episode=200, epsilon_floor=0.1):\n",
    "    if episode < floor_episode:\n",
    "        return 1.-(1.-epsilon_floor)*episode/floor_episode\n",
    "    else:\n",
    "        return epsilon_floor\n",
    "\n",
    "# dataset = get_dataset(dataset_dir,\n",
    "#                                      'validation',\n",
    "#                                      config.batch_size,\n",
    "#                                      image_h = config.image_hm,\n",
    "#                                      image_w = config.image_wm,\n",
    "#                                      preprocessing='identity',\n",
    "#                                      rggb_mode=False,\n",
    "#                                      central_squeeze_and_pad_factor=-1)\n",
    "\n",
    "# # env = RetinaEnv(config, image_generator=dataset)\n",
    "# env = RetinaEnv(config) #freezing on first batch\n",
    "# for images,_ in dataset:\n",
    "#     break\n",
    "    \n",
    "env =  SimpleMountainCarContinuous(max_steps=2000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# actor_model = create_actor_model(env.image_h, env.image_w,\n",
    "#                                  env.spectral_density_size, env.location_history_size,\n",
    "#                                  env.timestep_size, env.action_size)\n",
    "actor_model = create_actor_network_test_v1((2,), 2.0)\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "actor_model.optimizer = actor_optimizer\n",
    "agent = GaussianPolicyAgent(std_deviation=0.2,model=actor_model,action_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f073cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_vec = [0.1,0.5,1.0,2.0,4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f80899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma 0.1, percentage_done [%] 0.00, mean steps for dones nan████████████████████████████████████████-| 99.9% \n",
      " |----------------------------------------------------------------------------------------------------| 0.8% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3296847/746933288.py:56: RuntimeWarning: Mean of empty slice.\n",
      "  mean_step_done = -reward_records[done_flag].mean()\n",
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma 0.5, percentage_done [%] 0.00, mean steps for dones nan████████████████████████████████████████-| 99.9% \n",
      "sigma 1.0, percentage_done [%] 4.20, mean steps for dones 1227.33████████████████████████████████████-| 99.9% \n",
      "sigma 2.0, percentage_done [%] 71.40, mean steps for dones 956.69████████████████████████████████████-| 99.9% \n",
      "sigma 4.0, percentage_done [%] 99.70, mean steps for dones 365.48████████████████████████████████████-| 99.9% \n"
     ]
    }
   ],
   "source": [
    "for sigma in sigma_vec:\n",
    "# Training loop\n",
    "    reward_records = []\n",
    "    epsilon_records = []\n",
    "    action_mean_records = []\n",
    "    action_var_records = []\n",
    "    action_statmean_records = []\n",
    "    action_statvar_records = []\n",
    "    episodes = 1000\n",
    "    for ep in range(episodes):\n",
    "        prev_state = env.reset()\n",
    "        episodic_reward = 0\n",
    "        states, actions, rewards = [], [], []\n",
    "\n",
    "    #     epsilon = epsilon_scheduler(ep, floor_episode=1000)\n",
    "\n",
    "        while True:\n",
    "\n",
    "    #         deterministic_action, means,stdevs = agent.get_action(prev_state[np.newaxis,...],\n",
    "    #                                                return_stats=True)\n",
    "            random_action = [[sigma *np.random.normal()]]\n",
    "            action = random_action \n",
    "            #hook in order to add noise if neccessary\n",
    "    #         state, reward, done, info = env.step(np.reshape(action,[1]))\n",
    "            state, reward, done, info = env.step(action[0][0])\n",
    "\n",
    "    #         if env.warmup_done:\n",
    "            states.append([prev_state])\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "    #         action_mean_records.append(deterministic_action.mean(axis=0))\n",
    "    #         action_var_records.append(deterministic_action.var(axis=0))\n",
    "    #         action_statmean_records.append(means)\n",
    "    #         action_statvar_records.append(stdevs)\n",
    "            # End this episode when `done` is True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = np.copy(state)\n",
    "\n",
    "    #     agent.train(np.array(states), np.array(actions), np.array(rewards))\n",
    "        reward_records.append(episodic_reward)\n",
    "        print_progress_bar(ep, episodes)\n",
    "#         print(f\"Episode * {ep} * Episodic Reward is ==> {np.mean([episodic_reward])}\")\n",
    "    #     print(f\"Episode * {ep} * exploration epsilon {epsilon} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "    #     print(\"action means and variances at step -10:\", action_mean_records[-10],action_var_records[-10])\n",
    "    #     print(\"action means and variances at step -5:\", action_mean_records[-5],action_var_records[-5])\n",
    "    #     print(\"action statmeans and variances at step -10:\", action_statmean_records[-10][0],action_statvar_records[-10][0])\n",
    "    #     print(\"action statmeans and variances at step -5:\", action_statmean_records[-5][0],action_statvar_records[-5][0])\n",
    "    reward_records = np.array(reward_records)\n",
    "    done_flag = (-reward_records < env.max_steps)\n",
    "    percentage_done = done_flag.mean()\n",
    "    mean_step_done = -reward_records[done_flag].mean()\n",
    "    print('sigma {}, percentage_done [%] {:.2f}, mean steps for dones {:.2f}'.format(sigma, percentage_done*100, mean_step_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280e972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma 6.0, percentage_done [%] 100.00, mean steps for dones 185.27███████████████████████████████████-| 99.9% \n",
      "sigma 8.0, percentage_done [%] 100.00, mean steps for dones 118.12███████████████████████████████████-| 99.9% \n",
      "sigma 10.0, percentage_done [%] 100.00, mean steps for dones 76.55███████████████████████████████████-| 99.9% \n",
      "sigma 15, percentage_done [%] 100.00, mean steps for dones 60.99█████████████████████████████████████-| 99.9% \n"
     ]
    }
   ],
   "source": [
    "for sigma in [6., 8., 10.,15]:\n",
    "# Training loop\n",
    "    reward_records = []\n",
    "    epsilon_records = []\n",
    "    action_mean_records = []\n",
    "    action_var_records = []\n",
    "    action_statmean_records = []\n",
    "    action_statvar_records = []\n",
    "    episodes = 1000\n",
    "    for ep in range(episodes):\n",
    "        prev_state = env.reset()\n",
    "        episodic_reward = 0\n",
    "        states, actions, rewards = [], [], []\n",
    "\n",
    "    #     epsilon = epsilon_scheduler(ep, floor_episode=1000)\n",
    "\n",
    "        while True:\n",
    "\n",
    "    #         deterministic_action, means,stdevs = agent.get_action(prev_state[np.newaxis,...],\n",
    "    #                                                return_stats=True)\n",
    "            random_action = [[sigma *np.random.normal()]]\n",
    "            action = random_action \n",
    "            #hook in order to add noise if neccessary\n",
    "    #         state, reward, done, info = env.step(np.reshape(action,[1]))\n",
    "            state, reward, done, info = env.step(action[0][0])\n",
    "\n",
    "    #         if env.warmup_done:\n",
    "            states.append([prev_state])\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "    #         action_mean_records.append(deterministic_action.mean(axis=0))\n",
    "    #         action_var_records.append(deterministic_action.var(axis=0))\n",
    "    #         action_statmean_records.append(means)\n",
    "    #         action_statvar_records.append(stdevs)\n",
    "            # End this episode when `done` is True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = np.copy(state)\n",
    "\n",
    "    #     agent.train(np.array(states), np.array(actions), np.array(rewards))\n",
    "        reward_records.append(episodic_reward)\n",
    "        print_progress_bar(ep, episodes)\n",
    "#         print(f\"Episode * {ep} * Episodic Reward is ==> {np.mean([episodic_reward])}\")\n",
    "    #     print(f\"Episode * {ep} * exploration epsilon {epsilon} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "    #     print(\"action means and variances at step -10:\", action_mean_records[-10],action_var_records[-10])\n",
    "    #     print(\"action means and variances at step -5:\", action_mean_records[-5],action_var_records[-5])\n",
    "    #     print(\"action statmeans and variances at step -10:\", action_statmean_records[-10][0],action_statvar_records[-10][0])\n",
    "    #     print(\"action statmeans and variances at step -5:\", action_statmean_records[-5][0],action_statvar_records[-5][0])\n",
    "    reward_records = np.array(reward_records)\n",
    "    done_flag = (-reward_records < env.max_steps)\n",
    "    percentage_done = done_flag.mean()\n",
    "    mean_step_done = -reward_records[done_flag].mean()\n",
    "    print('sigma {}, percentage_done [%] {:.2f}, mean steps for dones {:.2f}'.format(sigma, percentage_done*100, mean_step_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3c3246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma 20.0, percentage_done [%] 100.00, mean steps for dones 69.53███████████████████████████████████-| 99.9% \n",
      "sigma 40.0, percentage_done [%] 100.00, mean steps for dones 113.77██████████████████████████████████-| 99.9% \n",
      "sigma 80, percentage_done [%] 99.90, mean steps for dones 198.62█████████████████████████████████████-| 99.9% \n"
     ]
    }
   ],
   "source": [
    "for sigma in [20.,40.,80]:\n",
    "# Training loop\n",
    "    reward_records = []\n",
    "    epsilon_records = []\n",
    "    action_mean_records = []\n",
    "    action_var_records = []\n",
    "    action_statmean_records = []\n",
    "    action_statvar_records = []\n",
    "    episodes = 1000\n",
    "    for ep in range(episodes):\n",
    "        prev_state = env.reset()\n",
    "        episodic_reward = 0\n",
    "        states, actions, rewards = [], [], []\n",
    "\n",
    "    #     epsilon = epsilon_scheduler(ep, floor_episode=1000)\n",
    "\n",
    "        while True:\n",
    "\n",
    "    #         deterministic_action, means,stdevs = agent.get_action(prev_state[np.newaxis,...],\n",
    "    #                                                return_stats=True)\n",
    "            random_action = [[sigma *np.random.normal()]]\n",
    "            action = random_action \n",
    "            #hook in order to add noise if neccessary\n",
    "    #         state, reward, done, info = env.step(np.reshape(action,[1]))\n",
    "            state, reward, done, info = env.step(action[0][0])\n",
    "\n",
    "    #         if env.warmup_done:\n",
    "            states.append([prev_state])\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            episodic_reward += reward\n",
    "\n",
    "    #         action_mean_records.append(deterministic_action.mean(axis=0))\n",
    "    #         action_var_records.append(deterministic_action.var(axis=0))\n",
    "    #         action_statmean_records.append(means)\n",
    "    #         action_statvar_records.append(stdevs)\n",
    "            # End this episode when `done` is True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            prev_state = np.copy(state)\n",
    "\n",
    "    #     agent.train(np.array(states), np.array(actions), np.array(rewards))\n",
    "        reward_records.append(episodic_reward)\n",
    "        print_progress_bar(ep, episodes)\n",
    "#         print(f\"Episode * {ep} * Episodic Reward is ==> {np.mean([episodic_reward])}\")\n",
    "    #     print(f\"Episode * {ep} * exploration epsilon {epsilon} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "    #     print(\"action means and variances at step -10:\", action_mean_records[-10],action_var_records[-10])\n",
    "    #     print(\"action means and variances at step -5:\", action_mean_records[-5],action_var_records[-5])\n",
    "    #     print(\"action statmeans and variances at step -10:\", action_statmean_records[-10][0],action_statvar_records[-10][0])\n",
    "    #     print(\"action statmeans and variances at step -5:\", action_statmean_records[-5][0],action_statvar_records[-5][0])\n",
    "    reward_records = np.array(reward_records)\n",
    "    done_flag = (-reward_records < env.max_steps)\n",
    "    percentage_done = done_flag.mean()\n",
    "    mean_step_done = -reward_records[done_flag].mean()\n",
    "    print('sigma {}, percentage_done [%] {:.2f}, mean steps for dones {:.2f}'.format(sigma, percentage_done*100, mean_step_done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17166c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uu in range(150):\n",
    "    for zz in range(1000000):\n",
    "        x = np.random.normal((50))\n",
    "    print_progress_bar(uu,149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed184f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
