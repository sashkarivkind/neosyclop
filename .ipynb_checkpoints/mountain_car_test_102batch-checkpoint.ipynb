{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f713d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batched toy environment for testing the policy gradient with batches'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''batched toy environment for testing the policy gradient with batches'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b430abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "from imagenet_dataset import get_dataset\n",
    "from retina_env import RetinaEnv, calculate_retinal_filter\n",
    "from rl_networks import create_actor_model, create_critic_model, policy\n",
    "from rl_core import Buffer, update_target\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pg_core import GaussianPolicyAgent\n",
    "from rl_networks import create_actor_network_test_v1\n",
    "\n",
    "from test_envs import BatchMountainCarContinuous \n",
    "\n",
    "import types\n",
    "config = types.SimpleNamespace()\n",
    "config.batch_size = 32\n",
    "# config.margin = 20\n",
    "# config.image_h = 224\n",
    "# config.image_w = 224\n",
    "# config.image_hm = config.image_h+2*config.margin\n",
    "# config.image_wm = config.image_w+2*config.margin\n",
    "# config.foveate = None\n",
    "# config.do_grayscale = True\n",
    "# config.history_length = 16\n",
    "# config.t_ignore = 16\n",
    "# config.t_max =50\n",
    "# config.motion_mode = 'velocity'\n",
    "# config.use_dones = False\n",
    "\n",
    "# config.gym_mode = False\n",
    "# t_vec = np.linspace(0,150,16)\n",
    "\n",
    "# balanced_filter = calculate_retinal_filter(t_vec, R=1.0)\n",
    "# config.filter = balanced_filter.reshape([1,1,-1,1])\n",
    "# config.min_freq = 1\n",
    "# config.max_freq = 13\n",
    "# config.action_upper_bound = np.array([2.0, 2.0])\n",
    "actor_lr = 1e-4\n",
    "dataset_dir = '/home/bnapp/datasets/tensorflow_datasets/imagenet2012/5.0.0/'\n",
    "\n",
    "def epsilon_scheduler(episode, floor_episode=200, epsilon_floor=0.1):\n",
    "    if episode < floor_episode:\n",
    "        return 1.-(1.-epsilon_floor)*episode/floor_episode\n",
    "    else:\n",
    "        return epsilon_floor\n",
    "\n",
    "# dataset = get_dataset(dataset_dir,\n",
    "#                                      'validation',\n",
    "#                                      config.batch_size,\n",
    "#                                      image_h = config.image_hm,\n",
    "#                                      image_w = config.image_wm,\n",
    "#                                      preprocessing='identity',\n",
    "#                                      rggb_mode=False,\n",
    "#                                      central_squeeze_and_pad_factor=-1)\n",
    "\n",
    "# # env = RetinaEnv(config, image_generator=dataset)\n",
    "# env = RetinaEnv(config) #freezing on first batch\n",
    "# for images,_ in dataset:\n",
    "#     break\n",
    "    \n",
    "env =  BatchMountainCarContinuous(batch_size=config.batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# actor_model = create_actor_model(env.image_h, env.image_w,\n",
    "#                                  env.spectral_density_size, env.location_history_size,\n",
    "#                                  env.timestep_size, env.action_size)\n",
    "actor_model = create_actor_network_test_v1((2,), 15.0)\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "actor_model.optimizer = actor_optimizer\n",
    "agent = GaussianPolicyAgent(std_deviation=10.,model=actor_model,action_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f80899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Episodic Reward is ==> -300.0\n",
      "Episode * 1 * Episodic Reward is ==> -300.0\n",
      "Episode * 2 * Episodic Reward is ==> -300.0\n",
      "Episode * 3 * Episodic Reward is ==> -288.71875\n",
      "Episode * 4 * Episodic Reward is ==> -300.0\n",
      "Episode * 5 * Episodic Reward is ==> -300.0\n",
      "Episode * 6 * Episodic Reward is ==> -300.0\n",
      "Episode * 7 * Episodic Reward is ==> -300.0\n",
      "Episode * 8 * Episodic Reward is ==> -300.0\n",
      "Episode * 9 * Episodic Reward is ==> -300.0\n",
      "Episode * 10 * Episodic Reward is ==> -300.0\n",
      "Episode * 11 * Episodic Reward is ==> -288.3125\n",
      "Episode * 12 * Episodic Reward is ==> -300.0\n",
      "Episode * 13 * Episodic Reward is ==> -300.0\n",
      "Episode * 14 * Episodic Reward is ==> -300.0\n",
      "Episode * 15 * Episodic Reward is ==> -300.0\n",
      "Episode * 16 * Episodic Reward is ==> -300.0\n",
      "Episode * 17 * Episodic Reward is ==> -300.0\n",
      "Episode * 18 * Episodic Reward is ==> -300.0\n",
      "Episode * 19 * Episodic Reward is ==> -300.0\n",
      "Episode * 20 * Episodic Reward is ==> -300.0\n",
      "Episode * 21 * Episodic Reward is ==> -277.125\n",
      "Episode * 22 * Episodic Reward is ==> -300.0\n",
      "Episode * 23 * Episodic Reward is ==> -288.3125\n",
      "Episode * 24 * Episodic Reward is ==> -300.0\n",
      "Episode * 25 * Episodic Reward is ==> -300.0\n",
      "Episode * 26 * Episodic Reward is ==> -300.0\n",
      "Episode * 27 * Episodic Reward is ==> -300.0\n",
      "Episode * 28 * Episodic Reward is ==> -300.0\n",
      "Episode * 29 * Episodic Reward is ==> -300.0\n",
      "Episode * 30 * Episodic Reward is ==> -300.0\n",
      "Episode * 31 * Episodic Reward is ==> -300.0\n",
      "Episode * 32 * Episodic Reward is ==> -276.53125\n",
      "Episode * 33 * Episodic Reward is ==> -300.0\n",
      "Episode * 34 * Episodic Reward is ==> -300.0\n",
      "Episode * 35 * Episodic Reward is ==> -300.0\n",
      "Episode * 36 * Episodic Reward is ==> -300.0\n",
      "Episode * 37 * Episodic Reward is ==> -300.0\n",
      "Episode * 38 * Episodic Reward is ==> -300.0\n",
      "Episode * 39 * Episodic Reward is ==> -300.0\n",
      "Episode * 40 * Episodic Reward is ==> -300.0\n",
      "Episode * 41 * Episodic Reward is ==> -300.0\n",
      "Episode * 42 * Episodic Reward is ==> -276.8125\n",
      "Episode * 43 * Episodic Reward is ==> -300.0\n",
      "Episode * 44 * Episodic Reward is ==> -300.0\n",
      "Episode * 45 * Episodic Reward is ==> -300.0\n",
      "Episode * 46 * Episodic Reward is ==> -300.0\n",
      "Episode * 47 * Episodic Reward is ==> -300.0\n",
      "Episode * 48 * Episodic Reward is ==> -300.0\n",
      "Episode * 49 * Episodic Reward is ==> -300.0\n",
      "Episode * 50 * Episodic Reward is ==> -300.0\n",
      "Episode * 51 * Episodic Reward is ==> -300.0\n",
      "Episode * 52 * Episodic Reward is ==> -300.0\n",
      "Episode * 53 * Episodic Reward is ==> -300.0\n",
      "Episode * 54 * Episodic Reward is ==> -300.0\n",
      "Episode * 55 * Episodic Reward is ==> -300.0\n",
      "Episode * 56 * Episodic Reward is ==> -300.0\n",
      "Episode * 57 * Episodic Reward is ==> -300.0\n",
      "Episode * 58 * Episodic Reward is ==> -300.0\n",
      "Episode * 59 * Episodic Reward is ==> -300.0\n",
      "Episode * 60 * Episodic Reward is ==> -288.125\n",
      "Episode * 61 * Episodic Reward is ==> -300.0\n",
      "Episode * 62 * Episodic Reward is ==> -288.53125\n",
      "Episode * 63 * Episodic Reward is ==> -300.0\n",
      "Episode * 64 * Episodic Reward is ==> -277.15625\n",
      "Episode * 65 * Episodic Reward is ==> -300.0\n",
      "Episode * 66 * Episodic Reward is ==> -300.0\n",
      "Episode * 67 * Episodic Reward is ==> -300.0\n",
      "Episode * 68 * Episodic Reward is ==> -300.0\n",
      "Episode * 69 * Episodic Reward is ==> -300.0\n",
      "Episode * 70 * Episodic Reward is ==> -300.0\n",
      "Episode * 71 * Episodic Reward is ==> -288.34375\n",
      "Episode * 72 * Episodic Reward is ==> -300.0\n",
      "Episode * 73 * Episodic Reward is ==> -300.0\n",
      "Episode * 74 * Episodic Reward is ==> -300.0\n",
      "Episode * 75 * Episodic Reward is ==> -300.0\n",
      "Episode * 76 * Episodic Reward is ==> -300.0\n",
      "Episode * 77 * Episodic Reward is ==> -300.0\n",
      "Episode * 78 * Episodic Reward is ==> -288.9375\n",
      "Episode * 79 * Episodic Reward is ==> -300.0\n",
      "Episode * 80 * Episodic Reward is ==> -300.0\n",
      "Episode * 81 * Episodic Reward is ==> -300.0\n",
      "Episode * 82 * Episodic Reward is ==> -288.75\n",
      "Episode * 83 * Episodic Reward is ==> -300.0\n",
      "Episode * 84 * Episodic Reward is ==> -300.0\n",
      "Episode * 85 * Episodic Reward is ==> -288.15625\n",
      "Episode * 86 * Episodic Reward is ==> -300.0\n",
      "Episode * 87 * Episodic Reward is ==> -300.0\n",
      "Episode * 88 * Episodic Reward is ==> -300.0\n",
      "Episode * 89 * Episodic Reward is ==> -300.0\n",
      "Episode * 90 * Episodic Reward is ==> -300.0\n",
      "Episode * 91 * Episodic Reward is ==> -300.0\n",
      "Episode * 92 * Episodic Reward is ==> -288.59375\n",
      "Episode * 93 * Episodic Reward is ==> -288.125\n",
      "Episode * 94 * Episodic Reward is ==> -300.0\n",
      "Episode * 95 * Episodic Reward is ==> -300.0\n",
      "Episode * 96 * Episodic Reward is ==> -300.0\n",
      "Episode * 97 * Episodic Reward is ==> -300.0\n",
      "Episode * 98 * Episodic Reward is ==> -300.0\n",
      "Episode * 99 * Episodic Reward is ==> -300.0\n",
      "Episode * 100 * Episodic Reward is ==> -300.0\n",
      "Episode * 101 * Episodic Reward is ==> -300.0\n",
      "Episode * 102 * Episodic Reward is ==> -300.0\n",
      "Episode * 103 * Episodic Reward is ==> -300.0\n",
      "Episode * 104 * Episodic Reward is ==> -288.5\n",
      "Episode * 105 * Episodic Reward is ==> -300.0\n",
      "Episode * 106 * Episodic Reward is ==> -300.0\n",
      "Episode * 107 * Episodic Reward is ==> -300.0\n",
      "Episode * 108 * Episodic Reward is ==> -300.0\n",
      "Episode * 109 * Episodic Reward is ==> -300.0\n",
      "Episode * 110 * Episodic Reward is ==> -289.0\n",
      "Episode * 111 * Episodic Reward is ==> -300.0\n",
      "Episode * 112 * Episodic Reward is ==> -300.0\n",
      "Episode * 113 * Episodic Reward is ==> -300.0\n",
      "Episode * 114 * Episodic Reward is ==> -288.28125\n",
      "Episode * 115 * Episodic Reward is ==> -300.0\n",
      "Episode * 116 * Episodic Reward is ==> -288.34375\n",
      "Episode * 117 * Episodic Reward is ==> -300.0\n",
      "Episode * 118 * Episodic Reward is ==> -300.0\n",
      "Episode * 119 * Episodic Reward is ==> -300.0\n",
      "Episode * 120 * Episodic Reward is ==> -300.0\n",
      "Episode * 121 * Episodic Reward is ==> -300.0\n",
      "Episode * 122 * Episodic Reward is ==> -300.0\n",
      "Episode * 123 * Episodic Reward is ==> -300.0\n",
      "Episode * 124 * Episodic Reward is ==> -300.0\n",
      "Episode * 125 * Episodic Reward is ==> -300.0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "reward_records = []\n",
    "epsilon_records = []\n",
    "action_mean_records = []\n",
    "action_var_records = []\n",
    "action_statmean_records = []\n",
    "action_statvar_records = []\n",
    "episodes = 10000\n",
    "for ep in range(episodes):\n",
    "    prev_states = env.reset()\n",
    "    episodic_rewards = 0\n",
    "    states_rec, actions_rec, rewards_rec, masks_rec = [], [], [], []\n",
    "\n",
    "#     epsilon = epsilon_scheduler(ep, floor_episode=1000)\n",
    "    masks = np.ones(config.batch_size)\n",
    "    while True:\n",
    "\n",
    "        deterministic_actions, means,stdevs = agent.get_action(prev_states,\n",
    "                                               return_stats=True)\n",
    "        actions = deterministic_actions #hook in order to add noise if neccessary\n",
    "#         state, reward, done, info = env.step(np.reshape(action,[1]))\n",
    "        states, rewards, dones, infos = env.step(actions)\n",
    "        \n",
    "#         if env.warmup_done:\n",
    "        states_rec.append(prev_states)\n",
    "        actions_rec.append(actions)\n",
    "        rewards_rec.append(rewards)\n",
    "        masks_rec.append(masks)\n",
    "        \n",
    "        episodic_rewards += rewards*masks\n",
    "\n",
    "        #after a single done step mask is being turned off\n",
    "        masks = np.logical_and(np.logical_not(dones), masks)\n",
    "        #after a single done step mask is being turned off\n",
    "\n",
    "\n",
    "        action_mean_records.append(deterministic_actions.mean(axis=0))\n",
    "        action_var_records.append(deterministic_actions.var(axis=0))\n",
    "        action_statmean_records.append(means)\n",
    "        action_statvar_records.append(stdevs)\n",
    "        # End this episode when `done` is True\n",
    "        if np.all(dones):\n",
    "            break\n",
    "\n",
    "        prev_states = np.copy(states)\n",
    "    \n",
    "    agent.train(np.array(states_rec), \n",
    "                np.array(actions_rec), \n",
    "                np.array(rewards_rec), \n",
    "                masks=np.array(masks_rec,dtype=np.bool),\n",
    "               max_iterations = 1000)\n",
    "    reward_records.append(episodic_rewards)\n",
    "    print(f\"Episode * {ep} * Episodic Reward is ==> {np.mean([episodic_rewards])}\")\n",
    "# #     print(f\"Episode * {ep} * exploration epsilon {epsilon} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "#     print(\"action means and variances at step -10:\", action_mean_records[-10],action_var_records[-10])\n",
    "#     print(\"action means and variances at step -5:\", action_mean_records[-5],action_var_records[-5])\n",
    "#     print(\"action statmeans and variances at step -10:\", action_statmean_records[-10][0],action_statvar_records[-10][0])\n",
    "#     print(\"action statmeans and variances at step -5:\", action_statmean_records[-5][0],action_statvar_records[-5][0])\n",
    "#     print(episodic_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc39e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16260854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(masks_rec,dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac31fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positions, velocities = env.states[:, 0:0], env.states[:, 1:1]\n",
    "positions, velocities = env.states[:, 0], env.states[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac81239",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocities += actions*env.force - env.gravity*np.cos(3*positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c11895",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocities += actions.squeeze()*env.force - env.gravity*np.cos(3*positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions*env.force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a89f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fdfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb51eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_state[np.newaxis,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(np.reshape(action,[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7383eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(action,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67488b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(action,[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f66aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "action[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea19088",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcca71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807436f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ddcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6668fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3149aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_records_=np.mean(reward_records, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c467bfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(action_statmean_records).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records)\n",
    "plt.plot(misc.smooth(reward_records,100))\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(reward_records)\n",
    "plt.plot(misc.smooth(reward_records,100))\n",
    "plt.grid()\n",
    "plt.ylim([50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e972a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
