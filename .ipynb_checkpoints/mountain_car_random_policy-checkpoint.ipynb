{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f713d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in this version we always use the same batch of images'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''in this version we always use the same batch of images'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b430abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "from imagenet_dataset import get_dataset\n",
    "from retina_env import RetinaEnv, calculate_retinal_filter\n",
    "from rl_networks import create_actor_model, create_critic_model, policy\n",
    "from rl_core import Buffer, update_target\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pg_core import GaussianPolicyAgent\n",
    "from rl_networks import create_actor_network_test_v1\n",
    "\n",
    "from test_envs import SimpleMountainCarContinuous \n",
    "\n",
    "import types\n",
    "config = types.SimpleNamespace()\n",
    "config.batch_size = 1\n",
    "# config.margin = 20\n",
    "# config.image_h = 224\n",
    "# config.image_w = 224\n",
    "# config.image_hm = config.image_h+2*config.margin\n",
    "# config.image_wm = config.image_w+2*config.margin\n",
    "# config.foveate = None\n",
    "# config.do_grayscale = True\n",
    "# config.history_length = 16\n",
    "# config.t_ignore = 16\n",
    "# config.t_max =50\n",
    "# config.motion_mode = 'velocity'\n",
    "# config.use_dones = False\n",
    "\n",
    "# config.gym_mode = False\n",
    "# t_vec = np.linspace(0,150,16)\n",
    "\n",
    "# balanced_filter = calculate_retinal_filter(t_vec, R=1.0)\n",
    "# config.filter = balanced_filter.reshape([1,1,-1,1])\n",
    "# config.min_freq = 1\n",
    "# config.max_freq = 13\n",
    "# config.action_upper_bound = np.array([2.0, 2.0])\n",
    "actor_lr = 1e-4\n",
    "dataset_dir = '/home/bnapp/datasets/tensorflow_datasets/imagenet2012/5.0.0/'\n",
    "\n",
    "def epsilon_scheduler(episode, floor_episode=200, epsilon_floor=0.1):\n",
    "    if episode < floor_episode:\n",
    "        return 1.-(1.-epsilon_floor)*episode/floor_episode\n",
    "    else:\n",
    "        return epsilon_floor\n",
    "\n",
    "# dataset = get_dataset(dataset_dir,\n",
    "#                                      'validation',\n",
    "#                                      config.batch_size,\n",
    "#                                      image_h = config.image_hm,\n",
    "#                                      image_w = config.image_wm,\n",
    "#                                      preprocessing='identity',\n",
    "#                                      rggb_mode=False,\n",
    "#                                      central_squeeze_and_pad_factor=-1)\n",
    "\n",
    "# # env = RetinaEnv(config, image_generator=dataset)\n",
    "# env = RetinaEnv(config) #freezing on first batch\n",
    "# for images,_ in dataset:\n",
    "#     break\n",
    "    \n",
    "env =  SimpleMountainCarContinuous()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# actor_model = create_actor_model(env.image_h, env.image_w,\n",
    "#                                  env.spectral_density_size, env.location_history_size,\n",
    "#                                  env.timestep_size, env.action_size)\n",
    "actor_model = create_actor_network_test_v1((2,), 2.0)\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "actor_model.optimizer = actor_optimizer\n",
    "agent = GaussianPolicyAgent(std_deviation=0.2,model=actor_model,action_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f80899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.08232409] [0.]\n",
      "action means and variances at step -5: [-0.00036009] [0.]\n",
      "action statmeans and variances at step -10: [-0.0311231] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03060859] [0.2]\n",
      "Episode * 1 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.16836678] [0.]\n",
      "action means and variances at step -5: [-0.10777155] [0.]\n",
      "action statmeans and variances at step -10: [-0.0423688] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04393242] [0.2]\n",
      "Episode * 2 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.42411413] [0.]\n",
      "action means and variances at step -5: [0.2064968] [0.]\n",
      "action statmeans and variances at step -10: [-0.03642796] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03820967] [0.2]\n",
      "Episode * 3 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.24365206] [0.]\n",
      "action means and variances at step -5: [-0.35677658] [0.]\n",
      "action statmeans and variances at step -10: [-0.03610493] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03724586] [0.2]\n",
      "Episode * 4 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.38089729] [0.]\n",
      "action means and variances at step -5: [-0.02381331] [0.]\n",
      "action statmeans and variances at step -10: [-0.04068503] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04255123] [0.2]\n",
      "Episode * 5 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.13142202] [0.]\n",
      "action means and variances at step -5: [0.00888603] [0.]\n",
      "action statmeans and variances at step -10: [-0.04951208] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04859202] [0.2]\n",
      "Episode * 6 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.07544376] [0.]\n",
      "action means and variances at step -5: [0.19537057] [0.]\n",
      "action statmeans and variances at step -10: [-0.05062942] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0496586] [0.2]\n",
      "Episode * 7 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.0294441] [0.]\n",
      "action means and variances at step -5: [-0.34211687] [0.]\n",
      "action statmeans and variances at step -10: [-0.0419337] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04158789] [0.2]\n",
      "Episode * 8 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.22533313] [0.]\n",
      "action means and variances at step -5: [-0.37443228] [0.]\n",
      "action statmeans and variances at step -10: [-0.04760211] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0470448] [0.2]\n",
      "Episode * 9 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.06037069] [0.]\n",
      "action means and variances at step -5: [-0.27019738] [0.]\n",
      "action statmeans and variances at step -10: [-0.04784447] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0513172] [0.2]\n",
      "Episode * 10 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.11139521] [0.]\n",
      "action means and variances at step -5: [0.04704482] [0.]\n",
      "action statmeans and variances at step -10: [-0.0422113] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04441898] [0.2]\n",
      "Episode * 11 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.12638786] [0.]\n",
      "action means and variances at step -5: [0.0204814] [0.]\n",
      "action statmeans and variances at step -10: [-0.04355644] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04639] [0.2]\n",
      "Episode * 12 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.1481354] [0.]\n",
      "action means and variances at step -5: [-0.20698588] [0.]\n",
      "action statmeans and variances at step -10: [-0.04830093] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0476708] [0.2]\n",
      "Episode * 13 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.09979549] [0.]\n",
      "action means and variances at step -5: [0.19053811] [0.]\n",
      "action statmeans and variances at step -10: [-0.04304022] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04297382] [0.2]\n",
      "Episode * 14 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.14362089] [0.]\n",
      "action means and variances at step -5: [-0.21417569] [0.]\n",
      "action statmeans and variances at step -10: [-0.03937705] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0433245] [0.2]\n",
      "Episode * 15 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.00478343] [0.]\n",
      "action means and variances at step -5: [0.26113207] [0.]\n",
      "action statmeans and variances at step -10: [-0.04677935] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04971375] [0.2]\n",
      "Episode * 16 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.03245843] [0.]\n",
      "action means and variances at step -5: [0.00645051] [0.]\n",
      "action statmeans and variances at step -10: [-0.04130325] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04065627] [0.2]\n",
      "Episode * 17 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.24854546] [0.]\n",
      "action means and variances at step -5: [0.12465362] [0.]\n",
      "action statmeans and variances at step -10: [-0.04114906] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04253116] [0.2]\n",
      "Episode * 18 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.0322224] [0.]\n",
      "action means and variances at step -5: [0.02641072] [0.]\n",
      "action statmeans and variances at step -10: [-0.03974177] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04057654] [0.2]\n",
      "Episode * 19 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.19608425] [0.]\n",
      "action means and variances at step -5: [0.03409597] [0.]\n",
      "action statmeans and variances at step -10: [-0.04025085] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03840579] [0.2]\n",
      "Episode * 20 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.38976682] [0.]\n",
      "action means and variances at step -5: [-0.27182893] [0.]\n",
      "action statmeans and variances at step -10: [-0.04198458] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04577572] [0.2]\n",
      "Episode * 21 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.05444153] [0.]\n",
      "action means and variances at step -5: [0.34182705] [0.]\n",
      "action statmeans and variances at step -10: [-0.03999522] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04145752] [0.2]\n",
      "Episode * 22 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.10854518] [0.]\n",
      "action means and variances at step -5: [-0.02508463] [0.]\n",
      "action statmeans and variances at step -10: [-0.04380434] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04463891] [0.2]\n",
      "Episode * 23 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.097544] [0.]\n",
      "action means and variances at step -5: [-0.27473031] [0.]\n",
      "action statmeans and variances at step -10: [-0.04356195] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04551824] [0.2]\n",
      "Episode * 24 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.04698682] [0.]\n",
      "action means and variances at step -5: [0.15850792] [0.]\n",
      "action statmeans and variances at step -10: [-0.04503389] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04509615] [0.2]\n",
      "Episode * 25 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.26764716] [0.]\n",
      "action means and variances at step -5: [0.0596646] [0.]\n",
      "action statmeans and variances at step -10: [-0.04688723] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05042978] [0.2]\n",
      "Episode * 26 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.20203814] [0.]\n",
      "action means and variances at step -5: [0.16830688] [0.]\n",
      "action statmeans and variances at step -10: [-0.04416337] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04757498] [0.2]\n",
      "Episode * 27 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.0386641] [0.]\n",
      "action means and variances at step -5: [0.0993498] [0.]\n",
      "action statmeans and variances at step -10: [-0.04399356] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04562537] [0.2]\n",
      "Episode * 28 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.42812267] [0.]\n",
      "action means and variances at step -5: [-0.02000657] [0.]\n",
      "action statmeans and variances at step -10: [-0.05134404] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05306095] [0.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 29 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.00580787] [0.]\n",
      "action means and variances at step -5: [-0.17946099] [0.]\n",
      "action statmeans and variances at step -10: [-0.04931892] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05148938] [0.2]\n",
      "Episode * 30 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.06020671] [0.]\n",
      "action means and variances at step -5: [0.0234718] [0.]\n",
      "action statmeans and variances at step -10: [-0.05060282] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05434507] [0.2]\n",
      "Episode * 31 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.33347265] [0.]\n",
      "action means and variances at step -5: [-0.05142159] [0.]\n",
      "action statmeans and variances at step -10: [-0.05654389] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05636239] [0.2]\n",
      "Episode * 32 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.20591477] [0.]\n",
      "action means and variances at step -5: [-0.13747706] [0.]\n",
      "action statmeans and variances at step -10: [-0.04949056] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05333402] [0.2]\n",
      "Episode * 33 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.07202578] [0.]\n",
      "action means and variances at step -5: [0.04215592] [0.]\n",
      "action statmeans and variances at step -10: [-0.04408063] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04433382] [0.2]\n",
      "Episode * 34 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.05517839] [0.]\n",
      "action means and variances at step -5: [0.18584219] [0.]\n",
      "action statmeans and variances at step -10: [-0.05007835] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05138218] [0.2]\n",
      "Episode * 35 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.47673703] [0.]\n",
      "action means and variances at step -5: [0.08861972] [0.]\n",
      "action statmeans and variances at step -10: [-0.05043086] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04895423] [0.2]\n",
      "Episode * 36 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.16614634] [0.]\n",
      "action means and variances at step -5: [0.11744693] [0.]\n",
      "action statmeans and variances at step -10: [-0.04807626] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05043491] [0.2]\n",
      "Episode * 37 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.10724432] [0.]\n",
      "action means and variances at step -5: [0.02863013] [0.]\n",
      "action statmeans and variances at step -10: [-0.04824354] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05042001] [0.2]\n",
      "Episode * 38 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.29056709] [0.]\n",
      "action means and variances at step -5: [0.12148951] [0.]\n",
      "action statmeans and variances at step -10: [-0.045557] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04558906] [0.2]\n",
      "Episode * 39 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.23337164] [0.]\n",
      "action means and variances at step -5: [-0.10374273] [0.]\n",
      "action statmeans and variances at step -10: [-0.05174221] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05322066] [0.2]\n",
      "Episode * 40 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.16556615] [0.]\n",
      "action means and variances at step -5: [-0.05379778] [0.]\n",
      "action statmeans and variances at step -10: [-0.044183] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04527136] [0.2]\n",
      "Episode * 41 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.0355722] [0.]\n",
      "action means and variances at step -5: [0.04711632] [0.]\n",
      "action statmeans and variances at step -10: [-0.05406691] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05189823] [0.2]\n",
      "Episode * 42 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.08333124] [0.]\n",
      "action means and variances at step -5: [0.00831672] [0.]\n",
      "action statmeans and variances at step -10: [-0.05042269] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04986976] [0.2]\n",
      "Episode * 43 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.06281092] [0.]\n",
      "action means and variances at step -5: [-0.02159398] [0.]\n",
      "action statmeans and variances at step -10: [-0.04874117] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05100319] [0.2]\n",
      "Episode * 44 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.06119409] [0.]\n",
      "action means and variances at step -5: [-0.10238365] [0.]\n",
      "action statmeans and variances at step -10: [-0.04837969] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04989569] [0.2]\n",
      "Episode * 45 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.40483877] [0.]\n",
      "action means and variances at step -5: [0.07705389] [0.]\n",
      "action statmeans and variances at step -10: [-0.04846516] [0.2]\n",
      "action statmeans and variances at step -5: [-0.05148508] [0.2]\n",
      "Episode * 46 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.17571983] [0.]\n",
      "action means and variances at step -5: [0.06316726] [0.]\n",
      "action statmeans and variances at step -10: [-0.04824319] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04919554] [0.2]\n",
      "Episode * 47 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.23192244] [0.]\n",
      "action means and variances at step -5: [-0.45063345] [0.]\n",
      "action statmeans and variances at step -10: [-0.04387457] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0437805] [0.2]\n",
      "Episode * 48 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.13722365] [0.]\n",
      "action means and variances at step -5: [0.05447078] [0.]\n",
      "action statmeans and variances at step -10: [-0.04827955] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04777086] [0.2]\n",
      "Episode * 49 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.04469688] [0.]\n",
      "action means and variances at step -5: [0.03126771] [0.]\n",
      "action statmeans and variances at step -10: [-0.04913207] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04890389] [0.2]\n",
      "Episode * 50 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.21718216] [0.]\n",
      "action means and variances at step -5: [-0.33484838] [0.]\n",
      "action statmeans and variances at step -10: [-0.04300817] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04325761] [0.2]\n",
      "Episode * 51 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.14997451] [0.]\n",
      "action means and variances at step -5: [0.27578511] [0.]\n",
      "action statmeans and variances at step -10: [-0.04001251] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04140029] [0.2]\n",
      "Episode * 52 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.09352134] [0.]\n",
      "action means and variances at step -5: [0.16981381] [0.]\n",
      "action statmeans and variances at step -10: [-0.04200731] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0424816] [0.2]\n",
      "Episode * 53 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.02866529] [0.]\n",
      "action means and variances at step -5: [-0.09996694] [0.]\n",
      "action statmeans and variances at step -10: [-0.04375388] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04501361] [0.2]\n",
      "Episode * 54 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.16188113] [0.]\n",
      "action means and variances at step -5: [-0.15387341] [0.]\n",
      "action statmeans and variances at step -10: [-0.04309489] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04353657] [0.2]\n",
      "Episode * 55 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.02611745] [0.]\n",
      "action means and variances at step -5: [-0.00795862] [0.]\n",
      "action statmeans and variances at step -10: [-0.0383819] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03992338] [0.2]\n",
      "Episode * 56 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.15062568] [0.]\n",
      "action means and variances at step -5: [-0.16143747] [0.]\n",
      "action statmeans and variances at step -10: [-0.03637815] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03540091] [0.2]\n",
      "Episode * 57 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.00696779] [0.]\n",
      "action means and variances at step -5: [-0.14147997] [0.]\n",
      "action statmeans and variances at step -10: [-0.03660463] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03832809] [0.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 58 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.07741182] [0.]\n",
      "action means and variances at step -5: [-0.02794497] [0.]\n",
      "action statmeans and variances at step -10: [-0.0413171] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04136436] [0.2]\n",
      "Episode * 59 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.01393605] [0.]\n",
      "action means and variances at step -5: [0.19964046] [0.]\n",
      "action statmeans and variances at step -10: [-0.04178935] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04116834] [0.2]\n",
      "Episode * 60 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.01781299] [0.]\n",
      "action means and variances at step -5: [0.15916269] [0.]\n",
      "action statmeans and variances at step -10: [-0.04099121] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04239585] [0.2]\n",
      "Episode * 61 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.19651015] [0.]\n",
      "action means and variances at step -5: [0.18042497] [0.]\n",
      "action statmeans and variances at step -10: [-0.03936018] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03751945] [0.2]\n",
      "Episode * 62 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.14793074] [0.]\n",
      "action means and variances at step -5: [-0.05197133] [0.]\n",
      "action statmeans and variances at step -10: [-0.04231529] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04574123] [0.2]\n",
      "Episode * 63 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.08626557] [0.]\n",
      "action means and variances at step -5: [0.15725421] [0.]\n",
      "action statmeans and variances at step -10: [-0.04318543] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04362816] [0.2]\n",
      "Episode * 64 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.10442533] [0.]\n",
      "action means and variances at step -5: [-0.40665548] [0.]\n",
      "action statmeans and variances at step -10: [-0.03725799] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03759648] [0.2]\n",
      "Episode * 65 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.09307485] [0.]\n",
      "action means and variances at step -5: [-0.15690123] [0.]\n",
      "action statmeans and variances at step -10: [-0.04154237] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04274608] [0.2]\n",
      "Episode * 66 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.09865979] [0.]\n",
      "action means and variances at step -5: [-0.19385716] [0.]\n",
      "action statmeans and variances at step -10: [-0.04015644] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04328692] [0.2]\n",
      "Episode * 67 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.2450324] [0.]\n",
      "action means and variances at step -5: [-0.2436944] [0.]\n",
      "action statmeans and variances at step -10: [-0.0401488] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04035319] [0.2]\n",
      "Episode * 68 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.29450135] [0.]\n",
      "action means and variances at step -5: [0.24749874] [0.]\n",
      "action statmeans and variances at step -10: [-0.0370463] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03855945] [0.2]\n",
      "Episode * 69 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.26174023] [0.]\n",
      "action means and variances at step -5: [-0.08843589] [0.]\n",
      "action statmeans and variances at step -10: [-0.03968581] [0.2]\n",
      "action statmeans and variances at step -5: [-0.04050934] [0.2]\n",
      "Episode * 70 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.16595842] [0.]\n",
      "action means and variances at step -5: [-0.46649672] [0.]\n",
      "action statmeans and variances at step -10: [-0.03902937] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03869122] [0.2]\n",
      "Episode * 71 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.03920544] [0.]\n",
      "action means and variances at step -5: [-0.27638097] [0.]\n",
      "action statmeans and variances at step -10: [-0.02929275] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03156038] [0.2]\n",
      "Episode * 72 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.3451682] [0.]\n",
      "action means and variances at step -5: [-0.07664825] [0.]\n",
      "action statmeans and variances at step -10: [-0.02914626] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03182814] [0.2]\n",
      "Episode * 73 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.03373523] [0.]\n",
      "action means and variances at step -5: [-0.1592162] [0.]\n",
      "action statmeans and variances at step -10: [-0.03227359] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0327585] [0.2]\n",
      "Episode * 74 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.18307717] [0.]\n",
      "action means and variances at step -5: [-0.203979] [0.]\n",
      "action statmeans and variances at step -10: [-0.03232495] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03334751] [0.2]\n",
      "Episode * 75 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.34130709] [0.]\n",
      "action means and variances at step -5: [-0.0388772] [0.]\n",
      "action statmeans and variances at step -10: [-0.03277807] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03207258] [0.2]\n",
      "Episode * 76 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.09576518] [0.]\n",
      "action means and variances at step -5: [0.02723343] [0.]\n",
      "action statmeans and variances at step -10: [-0.02776513] [0.2]\n",
      "action statmeans and variances at step -5: [-0.03144681] [0.2]\n",
      "Episode * 77 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.08124712] [0.]\n",
      "action means and variances at step -5: [0.09689442] [0.]\n",
      "action statmeans and variances at step -10: [-0.02744121] [0.2]\n",
      "action statmeans and variances at step -5: [-0.02802715] [0.2]\n",
      "Episode * 78 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.58356789] [0.]\n",
      "action means and variances at step -5: [0.22929902] [0.]\n",
      "action statmeans and variances at step -10: [-0.02466015] [0.2]\n",
      "action statmeans and variances at step -5: [-0.02513582] [0.2]\n",
      "Episode * 79 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.46365533] [0.]\n",
      "action means and variances at step -5: [-0.07480883] [0.]\n",
      "action statmeans and variances at step -10: [-0.02405147] [0.2]\n",
      "action statmeans and variances at step -5: [-0.0253432] [0.2]\n",
      "Episode * 80 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.18030626] [0.]\n",
      "action means and variances at step -5: [0.02351214] [0.]\n",
      "action statmeans and variances at step -10: [-0.02752794] [0.2]\n",
      "action statmeans and variances at step -5: [-0.02573469] [0.2]\n",
      "Episode * 81 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [0.08819293] [0.]\n",
      "action means and variances at step -5: [-0.15208391] [0.]\n",
      "action statmeans and variances at step -10: [-0.01908689] [0.2]\n",
      "action statmeans and variances at step -5: [-0.02066274] [0.2]\n",
      "Episode * 82 * Episodic Reward is ==> -300.0\n",
      "action means and variances at step -10: [-0.15702345] [0.]\n",
      "action means and variances at step -5: [0.54020698] [0.]\n",
      "action statmeans and variances at step -10: [-0.02285076] [0.2]\n",
      "action statmeans and variances at step -5: [-0.02427227] [0.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2796578/1580162879.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         deterministic_action, means,stdevs = agent.get_action(prev_state[np.newaxis,...],\n\u001b[0m\u001b[1;32m     19\u001b[0m                                                return_stats=True)\n\u001b[1;32m     20\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeterministic_action\u001b[0m \u001b[0;31m#hook in order to add noise if neccessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bnapp/arivkindNet/neosyclop/pg_core.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, states, return_stats)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# first two values are means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd_deviation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# learn std_dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1718\u001b[0m                         '. Consider setting it to AutoShardPolicy.DATA.')\n\u001b[1;32m   1719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1720\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1721\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1860\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1861\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4979\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4980\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4981\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4982\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4983\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3148\u001b[0m          \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[0;32m-> 3150\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   3151\u001b[0m         *args, **kwargs)\n\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m    921\u001b[0m       \u001b[0marg_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0mkwarg_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m     func_args = _get_defun_inputs_from_args(\n\u001b[0m\u001b[1;32m    924\u001b[0m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[1;32m    925\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_args\u001b[0;34m(args, names, flat_shapes)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_defun_inputs_from_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m   \u001b[0;34m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m   return _get_defun_inputs(\n\u001b[0m\u001b[1;32m   1160\u001b[0m       args, names, structure=args, flat_shapes=flat_shapes)\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[0;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mplaceholder_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m           placeholder = graph_placeholder(\n\u001b[0m\u001b[1;32m   1233\u001b[0m               \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaceholder_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m               name=requested_name)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/graph_only_ops.py\u001b[0m in \u001b[0;36mgraph_placeholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdtype_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   op = g._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0;34m\"Placeholder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       attrs=attrs, name=name)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         compute_device)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3559\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3561\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3562\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3563\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2055\u001b[0m       \u001b[0mtf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m       \u001b[0moutput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationOutputType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2057\u001b[0;31m       \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_with_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2058\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_with_tf_output\u001b[0;34m(op, value_index, dtype, tf_output)\u001b[0m\n\u001b[1;32m    403\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_with_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, op, value_index, dtype)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;31m# This will be set by self._as_tf_output().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ANY_TO_TF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m     \u001b[0;31m# TypeError indicates that type_value is not hashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "reward_records = []\n",
    "epsilon_records = []\n",
    "action_mean_records = []\n",
    "action_var_records = []\n",
    "action_statmean_records = []\n",
    "action_statvar_records = []\n",
    "episodes = 10000\n",
    "for ep in range(episodes):\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "#     epsilon = epsilon_scheduler(ep, floor_episode=1000)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        deterministic_action, means,stdevs = agent.get_action(prev_state[np.newaxis,...],\n",
    "                                               return_stats=True)\n",
    "        action = deterministic_action #hook in order to add noise if neccessary\n",
    "#         state, reward, done, info = env.step(np.reshape(action,[1]))\n",
    "        state, reward, done, info = env.step(action[0][0])\n",
    "        \n",
    "#         if env.warmup_done:\n",
    "        states.append([prev_state])\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episodic_reward += reward\n",
    "\n",
    "        action_mean_records.append(deterministic_action.mean(axis=0))\n",
    "        action_var_records.append(deterministic_action.var(axis=0))\n",
    "        action_statmean_records.append(means)\n",
    "        action_statvar_records.append(stdevs)\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = np.copy(state)\n",
    "    \n",
    "    agent.train(np.array(states), np.array(actions), np.array(rewards))\n",
    "    reward_records.append(episodic_reward)\n",
    "    print(f\"Episode * {ep} * Episodic Reward is ==> {np.mean([episodic_reward])}\")\n",
    "#     print(f\"Episode * {ep} * exploration epsilon {epsilon} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "#     print(\"action means and variances at step -10:\", action_mean_records[-10],action_var_records[-10])\n",
    "#     print(\"action means and variances at step -5:\", action_mean_records[-5],action_var_records[-5])\n",
    "#     print(\"action statmeans and variances at step -10:\", action_statmean_records[-10][0],action_statvar_records[-10][0])\n",
    "#     print(\"action statmeans and variances at step -5:\", action_statmean_records[-5][0],action_statvar_records[-5][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f94b707b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 1, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9834c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ae215",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_state[np.newaxis,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d857d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(np.reshape(action,[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(action,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.reshape(action,[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "action[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab2849",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c70cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b52d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66500fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6394cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d253fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3149aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_records_=np.mean(reward_records, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce240f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(action_statmean_records).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records_)\n",
    "plt.plot(misc.smooth(reward_records_,100))\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(reward_records)\n",
    "plt.plot(misc.smooth(reward_records_,100))\n",
    "plt.grid()\n",
    "plt.ylim([-15,-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(buffer.critic_loss_buffer[100:])\n",
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[100:],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(action_statmean_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11280265",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(action_statmean_records).reshape([-1,8])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353122d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(action_statmean_records).reshape([-1,8])[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_statmean_records[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e972a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
