{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b430abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.6.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bnapp/arivkindNet/neosyclop/imagenet_dataset.py:103: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '4'  # or any {'0', '1', '2'}\n",
    "from imagenet_dataset import get_dataset\n",
    "from retina_env import RetinaEnv, calculate_retinal_filter\n",
    "from rl_networks import create_actor_model, create_critic_model, policy\n",
    "from rl_core import Buffer, update_target\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import types\n",
    "config = types.SimpleNamespace()\n",
    "config.batch_size = 32\n",
    "config.margin = 20\n",
    "config.image_h = 224\n",
    "config.image_w = 224\n",
    "config.image_hm = config.image_h+2*config.margin\n",
    "config.image_wm = config.image_w+2*config.margin\n",
    "config.foveate = None\n",
    "config.do_grayscale = True\n",
    "config.history_length = 16\n",
    "config.t_ignore = 16\n",
    "config.t_max =50\n",
    "config.motion_mode = 'velocity'\n",
    "config.use_dones = True\n",
    "\n",
    "config.gym_mode = False\n",
    "t_vec = np.linspace(0,150,16)\n",
    "\n",
    "balanced_filter = calculate_retinal_filter(t_vec, R=1.0)\n",
    "config.filter = balanced_filter.reshape([1,1,-1,1])\n",
    "config.min_freq = 1\n",
    "config.max_freq = 13\n",
    "config.action_upper_bound = np.array([2.0, 2.0])\n",
    "\n",
    "dataset_dir = '/home/bnapp/datasets/tensorflow_datasets/imagenet2012/5.0.0/'\n",
    "\n",
    "def epsilon_scheduler(episode, floor_episode=200, epsilon_floor=0.1):\n",
    "    if episode < floor_episode:\n",
    "        return 1.-(1.-epsilon_floor)*episode/floor_episode\n",
    "    else:\n",
    "        return epsilon_floor\n",
    "\n",
    "dataset = get_dataset(dataset_dir,\n",
    "                                     'validation',\n",
    "                                     config.batch_size,\n",
    "                                     image_h = config.image_hm,\n",
    "                                     image_w = config.image_wm,\n",
    "                                     preprocessing='identity',\n",
    "                                     rggb_mode=False,\n",
    "                                     central_squeeze_and_pad_factor=-1)\n",
    "\n",
    "env = RetinaEnv(config, image_generator=dataset)\n",
    "\n",
    "if config.gym_mode:\n",
    "    num_states = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "\n",
    "    upper_bound = env.action_space.high[0]\n",
    "    lower_bound = env.action_space.low[0]\n",
    "else:\n",
    "    num_states = env.observation_size\n",
    "    num_actions = env.action_size\n",
    "    upper_bound = env.action_upper_bound\n",
    "    lower_bound = env.action_lower_bound\n",
    "\n",
    "# You might want to adjust the hyperparameters\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "gamma = 15./16.\n",
    "tau = 0.005\n",
    "\n",
    "buffer_capacity = 10000\n",
    "\n",
    "\n",
    "\n",
    "# Create actor and critic networks\n",
    "actor_model = create_actor_model(env.image_h, env.image_w, env.spectral_density_size, env.location_history_size, env.timestep_size, env.action_size)\n",
    "critic_model = create_critic_model(env.image_h, env.image_w, env.spectral_density_size, env.location_history_size, env.timestep_size, env.action_size)\n",
    "\n",
    "# Create target actor and critic networks\n",
    "target_actor = create_actor_model(env.image_h, env.image_w, env.spectral_density_size, env.location_history_size, env.timestep_size, env.action_size)\n",
    "target_critic = create_critic_model(env.image_h, env.image_w, env.spectral_density_size, env.location_history_size, env.timestep_size, env.action_size)\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "# Experience replay buffer\n",
    "buffer = Buffer(buffer_capacity, config.batch_size, num_states=num_states, num_actions=num_actions,\n",
    "                state_reshape_fn=env.unflatten_observation, use_dones=config.use_dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f80899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 0 * exploration epsilon 1.0 * Episodic Reward is ==> -10.267127990722656\n",
      "action means and variances at step -10: [-0.29886293 -0.17157948] [3.5023353e-05 3.9776714e-05]\n",
      "action means and variances at step -5: [-0.33278316 -0.23370846] [3.8726874e-05 6.1590814e-05]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 1 * exploration epsilon 0.9991 * Episodic Reward is ==> -12.52334213256836\n",
      "action means and variances at step -10: [-0.43175566 -0.48382324] [1.0600996e-04 8.6193380e-05]\n",
      "action means and variances at step -5: [-0.4351316  -0.52111036] [1.1364900e-04 7.0178554e-05]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 2 * exploration epsilon 0.9982 * Episodic Reward is ==> -11.890865325927734\n",
      "action means and variances at step -10: [-0.50586444 -0.6570226 ] [0.00020049 0.00015529]\n",
      "action means and variances at step -5: [-0.5121412 -0.6762837] [0.00017603 0.00013429]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 3 * exploration epsilon 0.9973 * Episodic Reward is ==> -11.537487030029297\n",
      "action means and variances at step -10: [-0.5765384 -0.7722436] [0.00018495 0.00022735]\n",
      "action means and variances at step -5: [-0.58398867 -0.7832396 ] [0.00020948 0.00022979]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 4 * exploration epsilon 0.9964 * Episodic Reward is ==> -12.690677642822266\n",
      "action means and variances at step -10: [-0.63018686 -0.868188  ] [0.00020271 0.00034152]\n",
      "action means and variances at step -5: [-0.6384145 -0.8862865] [0.00022084 0.00038273]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 5 * exploration epsilon 0.9955 * Episodic Reward is ==> -12.705106735229492\n",
      "action means and variances at step -10: [-0.69982815 -0.99030566] [0.00028676 0.00083009]\n",
      "action means and variances at step -5: [-0.714861 -1.004105] [0.00025803 0.00063287]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 6 * exploration epsilon 0.9946 * Episodic Reward is ==> -12.458202362060547\n",
      "action means and variances at step -10: [-0.8028225 -1.1056542] [0.00038584 0.0008644 ]\n",
      "action means and variances at step -5: [-0.8146792 -1.1244724] [0.00044723 0.00104043]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 7 * exploration epsilon 0.9937 * Episodic Reward is ==> -13.288127899169922\n",
      "action means and variances at step -10: [-0.8936693 -1.2613219] [0.00028085 0.0008056 ]\n",
      "action means and variances at step -5: [-0.9152733 -1.280623 ] [0.0003171  0.00097174]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 8 * exploration epsilon 0.9928 * Episodic Reward is ==> -11.653419494628906\n",
      "action means and variances at step -10: [-1.0465391 -1.3730911] [0.00042725 0.00096922]\n",
      "action means and variances at step -5: [-1.0563507 -1.3903354] [0.00047598 0.00117996]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 9 * exploration epsilon 0.9919 * Episodic Reward is ==> -11.960918426513672\n",
      "action means and variances at step -10: [-1.0984896 -1.5045073] [0.0006166  0.00137962]\n",
      "action means and variances at step -5: [-1.1008679 -1.5327018] [0.0005721 0.0012442]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 10 * exploration epsilon 0.991 * Episodic Reward is ==> -10.983566284179688\n",
      "action means and variances at step -10: [-1.1847483 -1.644614 ] [0.00040938 0.00124703]\n",
      "action means and variances at step -5: [-1.1942737 -1.6610763] [0.00036801 0.00114975]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 11 * exploration epsilon 0.9901 * Episodic Reward is ==> -13.570552825927734\n",
      "action means and variances at step -10: [-1.2914014 -1.738545 ] [0.00047154 0.00113818]\n",
      "action means and variances at step -5: [-1.3169813 -1.7500669] [0.0003838  0.00087656]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 12 * exploration epsilon 0.9892 * Episodic Reward is ==> -11.622238159179688\n",
      "action means and variances at step -10: [-1.3493415 -1.8200172] [0.00048251 0.00102698]\n",
      "action means and variances at step -5: [-1.359439  -1.8387071] [0.00035873 0.00080901]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 13 * exploration epsilon 0.9883 * Episodic Reward is ==> -11.667720794677734\n",
      "action means and variances at step -10: [-1.4047451 -1.9266542] [0.00052192 0.00122978]\n",
      "action means and variances at step -5: [-1.423862  -1.9362301] [0.00057616 0.00148002]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 14 * exploration epsilon 0.9874 * Episodic Reward is ==> -13.823110580444336\n",
      "action means and variances at step -10: [-1.4993584 -1.9965427] [3.8693732e-04 3.6139081e-05]\n",
      "action means and variances at step -5: [-1.4996351 -1.9975162] [5.3119962e-04 4.2401953e-05]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 15 * exploration epsilon 0.9865 * Episodic Reward is ==> -13.732305526733398\n",
      "action means and variances at step -10: [-1.5029666 -2.       ] [0.00060704 0.        ]\n",
      "action means and variances at step -5: [-1.5073055 -2.       ] [0.00040644 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 16 * exploration epsilon 0.9856 * Episodic Reward is ==> -11.262582778930664\n",
      "action means and variances at step -10: [-1.5837533 -2.       ] [0.00052653 0.        ]\n",
      "action means and variances at step -5: [-1.6016713 -2.       ] [0.00039591 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 17 * exploration epsilon 0.9847 * Episodic Reward is ==> -11.720897674560547\n",
      "action means and variances at step -10: [-1.6446528 -2.       ] [0.00051649 0.        ]\n",
      "action means and variances at step -5: [-1.6431963 -2.       ] [0.00044367 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 18 * exploration epsilon 0.9838 * Episodic Reward is ==> -12.152669906616211\n",
      "action means and variances at step -10: [-1.6418291 -2.       ] [0.0002905 0.       ]\n",
      "action means and variances at step -5: [-1.6648573 -2.       ] [0.00030846 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 19 * exploration epsilon 0.9829 * Episodic Reward is ==> -10.948974609375\n",
      "action means and variances at step -10: [-1.7986763 -2.       ] [0.00051524 0.        ]\n",
      "action means and variances at step -5: [-1.8255445 -2.       ] [0.00040702 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 20 * exploration epsilon 0.982 * Episodic Reward is ==> -12.790743827819824\n",
      "action means and variances at step -10: [-1.8800653 -2.       ] [0.00030357 0.        ]\n",
      "action means and variances at step -5: [-1.91564 -2.     ] [0.00024407 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 21 * exploration epsilon 0.9811 * Episodic Reward is ==> -13.438488006591797\n",
      "action means and variances at step -10: [-1.9332833 -2.       ] [0.00027097 0.        ]\n",
      "action means and variances at step -5: [-1.955739 -2.      ] [0.00023015 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 22 * exploration epsilon 0.9802 * Episodic Reward is ==> -13.944196701049805\n",
      "action means and variances at step -10: [-1.9867768 -2.       ] [0.0002695 0.       ]\n",
      "action means and variances at step -5: [-1.978896 -2.      ] [0.00023037 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 23 * exploration epsilon 0.9793 * Episodic Reward is ==> -12.946727752685547\n",
      "action means and variances at step -10: [-1.9536757 -2.       ] [0.00047246 0.        ]\n",
      "action means and variances at step -5: [-1.9803565 -2.       ] [0.00039065 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 24 * exploration epsilon 0.9784 * Episodic Reward is ==> -16.122222900390625\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 25 * exploration epsilon 0.9775 * Episodic Reward is ==> -11.476337432861328\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-1.9988364 -2.       ] [4.1973093e-05 0.0000000e+00]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 26 * exploration epsilon 0.9766 * Episodic Reward is ==> -11.2601957321167\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 27 * exploration epsilon 0.9757 * Episodic Reward is ==> -11.66779899597168\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 28 * exploration epsilon 0.9748 * Episodic Reward is ==> -11.8417387008667\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 29 * exploration epsilon 0.9739 * Episodic Reward is ==> -12.034187316894531\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 30 * exploration epsilon 0.973 * Episodic Reward is ==> -13.244243621826172\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 31 * exploration epsilon 0.9721 * Episodic Reward is ==> -11.798385620117188\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 32 * exploration epsilon 0.9712 * Episodic Reward is ==> -11.37182331085205\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 33 * exploration epsilon 0.9703 * Episodic Reward is ==> -12.1904878616333\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 34 * exploration epsilon 0.9694 * Episodic Reward is ==> -12.689095497131348\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 35 * exploration epsilon 0.9685 * Episodic Reward is ==> -9.759008407592773\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 36 * exploration epsilon 0.9676 * Episodic Reward is ==> -12.093253135681152\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 37 * exploration epsilon 0.9667 * Episodic Reward is ==> -10.820388793945312\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 38 * exploration epsilon 0.9658 * Episodic Reward is ==> -10.945123672485352\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 39 * exploration epsilon 0.9649 * Episodic Reward is ==> -11.109506607055664\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 40 * exploration epsilon 0.964 * Episodic Reward is ==> -12.32956600189209\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 41 * exploration epsilon 0.9631 * Episodic Reward is ==> -10.855286598205566\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 42 * exploration epsilon 0.9621999999999999 * Episodic Reward is ==> -11.574423789978027\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 43 * exploration epsilon 0.9613 * Episodic Reward is ==> -13.979509353637695\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 44 * exploration epsilon 0.9604 * Episodic Reward is ==> -9.983072280883789\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 45 * exploration epsilon 0.9595 * Episodic Reward is ==> -12.756288528442383\n",
      "action means and variances at step -10: [-2. -2.] [0. 0.]\n",
      "action means and variances at step -5: [-2. -2.] [0. 0.]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 46 * exploration epsilon 0.9586 * Episodic Reward is ==> -11.750005722045898\n",
      "action means and variances at step -10: [-1.8726717 -2.       ] [0.00155842 0.        ]\n",
      "action means and variances at step -5: [-1.7181687 -2.       ] [0.00107411 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "Episode * 47 * exploration epsilon 0.9577 * Episodic Reward is ==> -11.137475967407227\n",
      "action means and variances at step -10: [-1.7894773 -2.       ] [0.00140938 0.        ]\n",
      "action means and variances at step -5: [-1.8448476 -2.       ] [0.00140962 0.        ]\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "reward_records = []\n",
    "epsilon_records = []\n",
    "action_mean_records = []\n",
    "action_var_records = []\n",
    "episodes = 10000\n",
    "for ep in range(episodes):\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "    epsilon = epsilon_scheduler(ep, floor_episode=1000)\n",
    "\n",
    "    while True:\n",
    "        # tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "        \n",
    "        deterministic_action = policy(env.unflatten_observation(prev_state), actor_model, lower_bound, upper_bound)\n",
    "        random_action = -2+4*np.random.uniform(size=(config.batch_size,2))\n",
    "\n",
    "        if env.warmup_done:\n",
    "            action = epsilon*random_action + (1-epsilon)*deterministic_action\n",
    "        else:\n",
    "            action = random_action\n",
    "\n",
    "        # Recieve state and reward from environment\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if env.warmup_done:\n",
    "            if config.use_dones:\n",
    "                buffer.record((prev_state, action, reward, state, done))\n",
    "            else:   \n",
    "                buffer.record((prev_state, action, reward, state))\n",
    "                \n",
    "            episodic_reward += reward\n",
    "\n",
    "            buffer.learn(actor_model, target_actor, critic_model, target_critic, actor_optimizer, critic_optimizer, gamma, tau)\n",
    "            update_target(target_actor.variables, actor_model.variables, tau)\n",
    "            update_target(target_critic.variables, critic_model.variables, tau)\n",
    "            \n",
    "            action_mean_records.append(deterministic_action.numpy().mean(axis=0))\n",
    "            action_var_records.append(deterministic_action.numpy().var(axis=0))\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        #prev_state = state avoid assingment by reference:\n",
    "        prev_state = np.copy(state)\n",
    "#         print('debug action: ', action[0])\n",
    "\n",
    "    print(f\"Episode * {ep} * exploration epsilon {epsilon} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "    print(\"action means and variances at step -10:\", action_mean_records[-10],action_var_records[-10])\n",
    "    print(\"action means and variances at step -5:\", action_mean_records[-5],action_var_records[-5])\n",
    "    reward_records.append(episodic_reward.numpy().mean())\n",
    "    epsilon_records.append(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3149aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records)\n",
    "plt.plot(misc.smooth(reward_records,100))\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records)\n",
    "plt.plot(misc.smooth(reward_records,1000))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(buffer.critic_loss_buffer[100:])\n",
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[100:],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb73a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(buffer.critic_loss_buffer[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2391c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[100:],1))\n",
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[100:],100))\n",
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[100:],1000))\n",
    "plt.ylim([0,0.05])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[-500:],1))\n",
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[-500:],100))\n",
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[-500:],1000))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3584d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(buffer.actor_loss_buffer[100:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de88971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_action.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69036da",
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_action.numpy().var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10337396",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_action2 = policy(env.unflatten_observation(prev_state), actor_model, lower_bound, upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad155b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_action2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b1248",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy(env.unflatten_observation(0.0*prev_state), actor_model, lower_bound, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_state in range(-3,4):\n",
    "    for k_act in range(-3,4):\n",
    "        print(k_state,k_act,critic_model([env.unflatten_observation(k_state*prev_state),\n",
    "                      k_act*deterministic_action]).numpy()[:5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d66a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_state in range(-3,4):\n",
    "    for k_act in range(-3,4):\n",
    "        print(k_state,k_act,critic_model([env.unflatten_observation(k_state*state),\n",
    "                      k_act*deterministic_action]).numpy()[:5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac145bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------')\n",
    "print(critic_model([env.unflatten_observation(state),\n",
    "                      deterministic_action]).numpy()[:5].T)\n",
    "print(critic_model([env.unflatten_observation(state),\n",
    "                      -deterministic_action]).numpy()[:5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc27e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(15):\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('----------')\n",
    "    print(reward.numpy()[:5])\n",
    "    print(critic_model([env.unflatten_observation(state),\n",
    "                          deterministic_action]).numpy()[:5].T)\n",
    "    print(critic_model([env.unflatten_observation(state),\n",
    "                          -deterministic_action]).numpy()[:5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward.numpy()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e558c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for ii in range(55):\n",
    "    random_action = -2+4*np.random.uniform(size=(config.batch_size,2))\n",
    "    state, reward, done, info = env.step(random_action)\n",
    "    print('----------')\n",
    "    print(reward.numpy()[:5])\n",
    "    print(critic_model([env.unflatten_observation(state),\n",
    "                          deterministic_action]).numpy()[:5].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493bd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
