{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "from imagenet_dataset import get_dataset\n",
    "from retina_env import RetinaEnv, calculate_retinal_filter\n",
    "from rl_networks import create_actor_model, create_critic_model, policy\n",
    "from rl_core import Buffer, update_target\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pg_core import GaussianPolicyAgent\n",
    "\n",
    "import types\n",
    "config = types.SimpleNamespace()\n",
    "config.batch_size = 4\n",
    "config.margin = 20\n",
    "config.image_h = 224\n",
    "config.image_w = 224\n",
    "config.image_hm = config.image_h+2*config.margin\n",
    "config.image_wm = config.image_w+2*config.margin\n",
    "config.foveate = None\n",
    "config.do_grayscale = True\n",
    "config.history_length = 16\n",
    "config.t_ignore = 16\n",
    "config.t_max =50\n",
    "config.motion_mode = 'velocity'\n",
    "config.use_dones = False\n",
    "\n",
    "config.gym_mode = False\n",
    "t_vec = np.linspace(0,150,16)\n",
    "\n",
    "balanced_filter = calculate_retinal_filter(t_vec, R=1.0)\n",
    "config.filter = balanced_filter.reshape([1,1,-1,1])\n",
    "config.min_freq = 1\n",
    "config.max_freq = 13\n",
    "config.action_upper_bound = np.array([2.0, 2.0])\n",
    "actor_lr = 1e-4\n",
    "dataset_dir = '/home/bnapp/datasets/tensorflow_datasets/imagenet2012/5.0.0/'\n",
    "\n",
    "def epsilon_scheduler(episode, floor_episode=200, epsilon_floor=0.1):\n",
    "    if episode < floor_episode:\n",
    "        return 1.-(1.-epsilon_floor)*episode/floor_episode\n",
    "    else:\n",
    "        return epsilon_floor\n",
    "\n",
    "dataset = get_dataset(dataset_dir,\n",
    "                                     'validation',\n",
    "                                     config.batch_size,\n",
    "                                     image_h = config.image_hm,\n",
    "                                     image_w = config.image_wm,\n",
    "                                     preprocessing='identity',\n",
    "                                     rggb_mode=False,\n",
    "                                     central_squeeze_and_pad_factor=-1)\n",
    "\n",
    "env = RetinaEnv(config, image_generator=dataset)\n",
    "\n",
    "if config.gym_mode:\n",
    "    num_states = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.shape[0]\n",
    "\n",
    "    upper_bound = env.action_space.high[0]\n",
    "    lower_bound = env.action_space.low[0]\n",
    "else:\n",
    "    num_states = env.observation_size\n",
    "    num_actions = env.action_size\n",
    "    upper_bound = env.action_upper_bound\n",
    "    lower_bound = env.action_lower_bound\n",
    "\n",
    "\n",
    "actor_model = create_actor_model(env.image_h, env.image_w,\n",
    "                                 env.spectral_density_size, env.location_history_size,\n",
    "                                 env.timestep_size, env.action_size)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "actor_model.optimizer = actor_optimizer\n",
    "agent = GaussianPolicyAgent(std_deviation=1.0,model=actor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f80899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "reward_records = []\n",
    "epsilon_records = []\n",
    "action_mean_records = []\n",
    "action_var_records = []\n",
    "action_statmean_records = []\n",
    "action_statvar_records = []\n",
    "episodes = 10000\n",
    "for ep in range(episodes):\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "#     epsilon = epsilon_scheduler(ep, floor_episode=1000)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        deterministic_action, means,stdevs = agent.get_action(env.unflatten_observation(prev_state),\n",
    "                                               return_stats=True)\n",
    "        action = deterministic_action #hook in order to add noise if neccessary\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if env.warmup_done:\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "                \n",
    "            episodic_reward += reward\n",
    "           \n",
    "            action_mean_records.append(deterministic_action.mean(axis=0))\n",
    "            action_var_records.append(deterministic_action.var(axis=0))\n",
    "            action_statmean_records.append(means)\n",
    "            action_statvar_records.append(stdevs)\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = np.copy(state)\n",
    "    \n",
    "    agent.train(np.array(states), np.array(actions), np.array(rewards),shaper_fn=env.unflatten_observation_v2)\n",
    "    reward_records.append(episodic_reward)\n",
    "    print(f\"Episode * {ep} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "#     print(f\"Episode * {ep} * exploration epsilon {epsilon} * Episodic Reward is ==> {episodic_reward.numpy().mean()}\")\n",
    "    print(\"action means and variances at step -10:\", action_mean_records[-10],action_var_records[-10])\n",
    "    print(\"action means and variances at step -5:\", action_mean_records[-5],action_var_records[-5])\n",
    "    print(\"action statmeans and variances at step -10:\", action_statmean_records[-10][0],action_statvar_records[-10][0])\n",
    "    print(\"action statmeans and variances at step -5:\", action_statmean_records[-5][0],action_statvar_records[-5][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3149aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_records_=np.mean(reward_records, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(action_statmean_records).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a74432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3b6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records_)\n",
    "plt.plot(misc.smooth(reward_records_,100))\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aab1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(reward_records)\n",
    "plt.plot(misc.smooth(reward_records_,100))\n",
    "plt.grid()\n",
    "plt.ylim([-15,-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(buffer.critic_loss_buffer[100:])\n",
    "plt.plot(misc.smooth(buffer.critic_loss_buffer[100:],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(action_statmean_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11280265",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(action_statmean_records).reshape([-1,8])[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353122d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(action_statmean_records).reshape([-1,8])[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_statmean_records[-300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e972a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
